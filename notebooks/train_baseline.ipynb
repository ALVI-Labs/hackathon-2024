{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import torch\n",
    "\n",
    "from train import TrainConfig, run_train_model\n",
    "from utils.augmentations import get_default_transform\n",
    "from models import hvatnet\n",
    "from utils import creating_dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): \n",
    "    n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n_total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total: {n_total/1e6:.2f}M, Trainable: {n_trainable/1e6:.2f}M\")\n",
    "    return n_total, n_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting val datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Getting train datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Number of trainining sessions: 1\n",
      "Number of validation sessions: 1\n",
      "Size of the input (8, 256) || Size of the output (20, 32)\n"
     ]
    }
   ],
   "source": [
    "train_config = TrainConfig(exp_name='test_2_run_fedya', p_augs=0.3, batch_size=64, eval_interval=150)\n",
    "    \n",
    "## Data preparation\n",
    "transform = get_default_transform(train_config.p_augs)\n",
    "data_paths = dict(datasets=[r\"D:\\Work\\alvi_labs\\code\\data\\processed\\dataset_v1_big\", \n",
    "                           r\"D:\\Work\\alvi_labs\\code\\data\\processed\\dataset_v2_blocks\"],\n",
    "                    hand_type = ['left', 'right'], # [left, 'right']\n",
    "                    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "                    test_dataset_list = ['fedya_tropin_standart_elbow_left'])\n",
    "data_config = creating_dataset.DataConfig(**data_paths)\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4210788\n",
      "Total: 4.21M, Trainable: 4.21M\n",
      "torch.Size([4, 20, 32])\n"
     ]
    }
   ],
   "source": [
    "## Init model\n",
    "model_config = hvatnet.Config(n_electrodes=8, n_channels_out=20,\n",
    "                            n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                            n_filters=128, kernel_size=3,\n",
    "                            strides=(2, 2, 2), dilation=2, \n",
    "                            small_strides = (2, 2))\n",
    "model = hvatnet.HVATNetv3(model_config)\n",
    "count_parameters(model)\n",
    "\n",
    "x = torch.zeros(4, 8, 256)\n",
    "y = model(x)\n",
    "\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoval_alvi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebed233148d44c6bbd97b22bdc994548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Work\\alvi_labs\\code\\hackathon-2024\\notebooks\\wandb\\run-20240513_173122-l2yis101</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koval_alvi/hvatnet-hackathon/runs/l2yis101' target=\"_blank\">test_2_run_fedya</a></strong> to <a href='https://wandb.ai/koval_alvi/hvatnet-hackathon' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koval_alvi/hvatnet-hackathon' target=\"_blank\">https://wandb.ai/koval_alvi/hvatnet-hackathon</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koval_alvi/hvatnet-hackathon/runs/l2yis101' target=\"_blank\">https://wandb.ai/koval_alvi/hvatnet-hackathon/runs/l2yis101</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed initialization of scheduler\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 150: 0.24392080307006836\n",
      "val loss: 0.3187941908836365\n",
      "saved model:  step_150_loss_0.3188.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 300: 0.2432033270597458\n",
      "val loss: 0.2892138659954071\n",
      "saved model:  step_300_loss_0.2892.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 450: 0.23250322043895721\n",
      "val loss: 0.2859196364879608\n",
      "saved model:  step_450_loss_0.2859.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 600: 0.23233862221240997\n",
      "val loss: 0.27606943249702454\n",
      "saved model:  step_600_loss_0.2761.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 750: 0.21094445884227753\n",
      "val loss: 0.2800562083721161\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 900: 0.21324972808361053\n",
      "val loss: 0.27461209893226624\n",
      "saved model:  step_900_loss_0.2746.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1050: 0.19171850383281708\n",
      "val loss: 0.26302289962768555\n",
      "saved model:  step_1050_loss_0.2630.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1200: 0.17333747446537018\n",
      "val loss: 0.2586509585380554\n",
      "saved model:  step_1200_loss_0.2587.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1350: 0.16292086243629456\n",
      "val loss: 0.27503105998039246\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1500: 0.1600300371646881\n",
      "val loss: 0.2628258168697357\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1650: 0.14265429973602295\n",
      "val loss: 0.2602822780609131\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1800: 0.14967115223407745\n",
      "val loss: 0.2594718933105469\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1950: 0.13991956412792206\n",
      "val loss: 0.2605088949203491\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2100: 0.15416193008422852\n",
      "val loss: 0.26991429924964905\n",
      "\n",
      "\n",
      "***********************************************************************************************************"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m merged_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m}\n\u001b[0;32m      4\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhvatnet-hackathon\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      5\u001b[0m            entity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkoval_alvi\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      6\u001b[0m            config\u001b[38;5;241m=\u001b[39mmerged_config,\n\u001b[0;32m      7\u001b[0m            name\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mexp_name)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mrun_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Work\\alvi_labs\\code\\hackathon-2024\\train.py:116\u001b[0m, in \u001b[0;36mrun_train_model\u001b[1;34m(model, datasets, config, device)\u001b[0m\n\u001b[0;32m    113\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mgrad_clip:\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    119\u001b[0m overall_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aleksandr.kovalev\\anaconda3\\envs\\torch_2\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), [grads]) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(grads, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[1;32m---> 76\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "merged_config = {**train_config.__dict__, **model_config.__dict__}\n",
    "\n",
    "wandb.init(project='hvatnet-hackathon', \n",
    "           entity='koval_alvi', \n",
    "           config=merged_config,\n",
    "           name=train_config.exp_name)\n",
    "\n",
    "run_train_model(model, (train_dataset, test_dataset), train_config, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
